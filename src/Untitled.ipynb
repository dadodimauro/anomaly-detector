{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f56f258-9297-43a9-b137-4ca702ef2ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/anomaly-detector\n"
     ]
    }
   ],
   "source": [
    "%cd /notebooks/anomaly-detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd1a76d2-139c-4451-b309-7e062d28ea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    from src.utils import utils\n",
    "    from src.utils import params\n",
    "    from src.utils import thresholds as th\n",
    "    from src.models import usad\n",
    "    from src.models import usad_utils\n",
    "    from src.data import columns\n",
    "    from src.data import preprocessing\n",
    "    from src.visualization import plotter\n",
    "except ModuleNotFoundError:\n",
    "    print(\"installing requirements..\")\n",
    "    os.system('pip install -r requirements.txt')\n",
    "    from src.utils import utils\n",
    "    from src.utils import params\n",
    "    from src.utils import thresholds as th\n",
    "    from src.models import usad\n",
    "    from src.models import usad_utils\n",
    "    from src.data import columns\n",
    "    from src.data import preprocessing\n",
    "    from src.visualization import plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5cfaf00-9bd6-41a0-b979-f5bc0a31cc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPROCESSING_PARAMS: {'downsamplig_rate': 5, 'window_size': 12, 'normalization': 'all', 'metrics': 'columns_6', 'multi_file': False}\n",
      "TRAINING_PARAMS: {'batch_size': 32, 'epochs': 3, 'hidden_size': 10, 'alpha': 0.5, 'beta': 0.5}\n",
      "INTERVALS_PARAMS: {'train_start': 40320, 'train_end': 70560, 'test_start': 0, 'test_end': 30240}\n",
      "TH_ALGORITHM: iqr\n",
      "DF_PATH: /notebooks/anomaly-detector/data/raw/V_GV_SYSMETRIC_INCTANCE_2.csv\n",
      "PLOT_PARAMS: {'history_static': True, 'history_html': False, 'db_time_static': True, 'db_time_html': False, 'labels_static': True, 'labels_html': False, 'thresholds_static': True, 'thresholds_html': True}\n"
     ]
    }
   ],
   "source": [
    "PREPROCESSING_PARAMS, TRAINING_PARAMS, INTERVALS_PARAMS, TH_ALGORITHM, PLOT_PARAMS, df_path = params.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4845fd6d-6e65-4513-b63e-a69f056ab2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multivariate_ad():\n",
    "    def __init__(self, PREPROCESSING_PARAMS, TRAINING_PARAMS, INTERVALS_PARAMS, \n",
    "                         TH_ALGORITHM, PLOT_PARAMS, df_path):\n",
    "        \n",
    "        self.PREPROCESSING_PARAMS = PREPROCESSING_PARAMS\n",
    "        # self.TRAINING_PARAMS = TRAINING_PARAMS\n",
    "        self.INTERVALS_PARAMS = INTERVALS_PARAMS\n",
    "        self.TH_ALGORITHM = TH_ALGORITHM\n",
    "        self.PLOT_PARAMS\n",
    "        self.df_path = df_path\n",
    "        \n",
    "        self.batch_size = TRAINING_PARAMS['batch_size']\n",
    "        self.epochs = TRAINING_PARAMS['epochs']\n",
    "        self.hidden_size = TRAINING_PARAMS['hidden_size']\n",
    "        self.single_file = PREPROCESSING_PARAMS['multi_file']\n",
    "        self.scaler = PREPROCESSING_PARAMS['normalization']\n",
    "        self.columns_name = PREPROCESSING_PARAMS['metrics']\n",
    "        self.alpha =  TRAINING_PARAMS['alpha']\n",
    "        self.beta =  TRAINING_PARAMS['beta']\n",
    "        \n",
    "        self.df = None\n",
    "        self.train_df = None\n",
    "        self.test_df = None\n",
    "\n",
    "        self.w_size = windows_train.shape[1] * windows_train.shape[2]\n",
    "        self.z_size = windows_train.shape[1] * hidden_size\n",
    "        self.device = utils.get_default_device()\n",
    "        \n",
    "        self.labels_ = None\n",
    "        self.decision_scores_ = None\n",
    "        self.anomalies_intervals_ = None\n",
    "        \n",
    "        \n",
    "    def get_data(self, preprocessing=True):\n",
    "        if preprocessing: scaler = self.scaler\n",
    "        else: scaler = None\n",
    "        \n",
    "        if self.single_file is True:\n",
    "            self.df = preprocessing.get_df(self.df_path, columns_name=self.columns_name)\n",
    "            return preprocessing.data_preprocessing(\n",
    "                                                    self.PREPROCESSING_PARAMS, self.df, \n",
    "                                                    INTERVALS_PARAMS=self.INTERVALS_PARAMS, \n",
    "                                                    scaler=scaler, multi=True, single_file=False\n",
    "                                                )\n",
    "\n",
    "        else:\n",
    "            self.train_df = preprocessing.get_df(self.df_train_path, columns_name=self.columns_name)\n",
    "            self.test_df = preprocessing.get_df(self.df_test_path, columns_name=self.columns_name)\n",
    "            return preprocessing.data_preprocessing(\n",
    "                                                    self.PREPROCESSING_PARAMS, df=None, \n",
    "                                                    INTERVALS_PARAMS=self.INTERVALS_PARAMS, \n",
    "                                                    scaler=scaler, multi=True,\n",
    "                                                    single_file=True, train_df=self.train_df, test_df=self.test_df\n",
    "                                                )\n",
    "        \n",
    "    def get_db_time(self):\n",
    "        if self.single_file is True:\n",
    "            _, db_time = preprocessing.get_db_time(self.df, self.PREPROCESSING_PARAMS,\n",
    "                                                           INTERVALS_PARAMS=self.INTERVALS_PARAMS, multi=True,\n",
    "                                                           single_file=True)\n",
    "        else:\n",
    "             db_time = preprocessing.get_db_time(self.test_df, self.PREPROCESSING_PARAMS, \n",
    "                                                    INTERVALS_PARAMS=self.INTERVALS_PARAMS, multi=False,\n",
    "                                                    single_file=False)\n",
    "                \n",
    "        return db_time\n",
    "        \n",
    "    # TODO: add fit and predict\n",
    "    def fit_predict(self, plot=True):\n",
    "        df_train, df_test, windows_train, windows_test, train_timestamps, test_timestamps = self.get_data()\n",
    "        db_time = self.get_db_time()\n",
    "        _, original_test_data, _, _, _, _ = self.get_data(preprocessing=False)\n",
    "        train_loader, val_loader, test_loader = preprocessing.get_dataloaders(\n",
    "                                                                    windows_train, windows_test, \n",
    "                                                                    self.batch_size, self.w_size, self.z_size\n",
    "                                                                )\n",
    "        model = usad.UsadModel(self.w_size, self.z_size)\n",
    "        model = utils.to_device(model, aelf.device)\n",
    "        history = usad_utils.training(self.epochs, model, self.device, train_loader, val_loader)\n",
    "        usad_utils.save_model(model)\n",
    "        model = usad_utils.load_checkpoint(model)\n",
    "        \n",
    "        results = usad_utils.test_model(model, self.device, test_loader, alpha=self.alpha, beta=self.beta)\n",
    "        y_pred = usad_utils.get_prediction_score(results)\n",
    "        labels = th.get_labels(self.TH_ALGORITHM, y_pred) \n",
    "        \n",
    "        anomalies_intervals_df = utils.generate_anomalies_intervals(labels, test_timestamps)\n",
    "        utils.save_anomalies_intervals(anomalies_intervals_df)\n",
    "        \n",
    "        # update public variables\n",
    "        self.decision_scores_ = y_pred\n",
    "        self.labels_ = labels\n",
    "        self.anomalies_intervals_ = anomalies_intervals_df\n",
    "        \n",
    "        if plot is True:\n",
    "            plotter.plot_history(history, save_static=PLOT_PARAMS['history_static'], save_html=PLOT_PARAMS['history_html'])\n",
    "            plotter.plot_res_db_time(y_pred, db_time, timestamps=test_timestamps, \n",
    "                                             save_static=PLOT_PARAMS['db_time_static'], save_html=PLOT_PARAMS['db_time_html'])\n",
    "            plotter.plot_labels(y_pred, labels, timestamps=test_timestamps, \n",
    "                                        save_static=PLOT_PARAMS['labels_static'], save_html=PLOT_PARAMS['labels_html'])\n",
    "            plotter.plot_thresholds(original_test_data, labels, timestamps=test_timestamps,\n",
    "                                            save_static=PLOT_PARAMS['thresholds_static'], save_html=PLOT_PARAMS['thresholds_html'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
